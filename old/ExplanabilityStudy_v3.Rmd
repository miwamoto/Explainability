---
title: "Explanability Study"
author: "Michael Amodeo, Krista Mar, Mona Iwamoto"
date: \today
output: pdf_document

---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

# load packages 
library(data.table)
library(foreign)
library(lmtest)
library(sandwich)
library(multiwayvcov)
library(stargazer)
library(AER)
```
## Data Import and Prepration

### Import data
In order to appropriately capture the instances where someone did not finish the survey, whether by clicking through to the final page or by stopping midway, I had to export from Qualtrics in a different format that allows us to see what was viewed and what was not. That also changed some other fields, particularly the multiple choice, multiple selection questions.

```{r}
#Import all data
all_content = readLines("Explainability_Study_legacy_export.csv")

#Delete second and third rows of not useful information
skip_second = all_content[-c(2,3)]

#Create table and data.table
d <-read.csv(textConnection(skip_second), header = TRUE, stringsAsFactors = FALSE)
d <- data.table(d)

# Create new data table without fields we are not using
dt <- d[,-c('ResponseSet','IPAddress','StartDate','EndDate','RecipientLastName',
          'RecipientFirstName','RecipientEmail','ExternalDataReference','Status',
          'Q_TotalDuration','Enter.Embedded.Data.Field.Name.Here...','LocationLatitude',
          'LocationLongitude','LocationAccuracy', 'Q3.5', 'Q4.5', 'Q6.5', 'Q7.5', 'Q8.1', 
          'Q9.1','Q10.1', 'Q10.3')]

# Rename variables
old_names <- colnames(dt)

##  Key to var names: tc = Twitter control group
#                     tt = Twitter treatment group
#                     rc = recidivism control group 
#                     rt = recidivism treatment group

new_names <- c("ResponseID","Finished","First.Context","random","intro","tweet",
               "tControl", 'tcFair', 'tcAcc', 'tcSat', 'tcUseful', 'tcClear', 
               'tcMeaningful', 'tcReqInfo', 'tcOther1', 'tcOther2', 'tcOther3', 'tcOther4',
               "tTreat", 'ttFair', 'ttAcc', 'ttSat', 'ttUseful', 'ttClear',
               'ttMeaningful', 'ttReqInfo', 'ttOther1', 'ttOther2', 'ttOther3', 'ttOther4',
               'recidivism', 
               'rControl', 'rcFair', 'rcAcc', 'rcSat', 'rcUseful', 'rcClear', 
               'rcMeaningful', 'rcReqInfo', 'rcOther1', 'rcOther2', 'rcOther3', 'rcOther4',
               "rTreat", 'rtFair', 'rtAcc', 'rtSat', 'rtUseful', 'rtClear',
               'rtMeaningful', 'rtReqInfo', 'rtOther1', 'rtOther2', 'rtOther3', 'rtOther4',
               'ageGroup', 'white', 'black', 'native', 'asian', 'pac_isle', 'hispanic', 
               'other', 'gender', 'socMed', 'educ', 'feedback')

setnames(dt, old_names, new_names)
colnames(dt)
```


```{r}
#summary(dt)
```

### Data Cleanup
```{r results = 'hide'}
# Most Qualtrics questions were set so the extreme positive value was the first choice (1)
# Function to flip the scale to show more positive as larger number
flip <- function(originalScale) {
  x <- originalScale - 3        #  3 is median
  return(3 - x)
}

# For an unknown reason, question 7.2 Fairness for Recidivism Treatment 
# the values were offset by 24.  This was cross-checked with the text-based responses.

dt$rtFair <- dt$rtFair - 24      # Qualtrics weirdness

# For the Twitter fairness questions, the Qualtrics survey 
# responses were reversed in two instances. All others
# are reversed using the flip function below

# Organize scales so larger values correlate to more fair, more accurate, etc.
dt[, tcAcc        := flip(dt[, tcAcc])]
dt[, tcSat        := flip(dt[, tcSat])]
dt[, tcUseful     := flip(dt[, tcUseful])]
dt[, tcClear      := flip(dt[, tcClear])]
dt[, tcMeaningful := flip(dt[, tcMeaningful])]

dt[, ttFair       := flip(dt[, ttFair])]
dt[, ttAcc        := flip(dt[, ttAcc])]
dt[, ttSat        := flip(dt[, ttSat])]
dt[, ttUseful     := flip(dt[, ttUseful])]
dt[, ttClear      := flip(dt[, ttClear])]
dt[, ttMeaningful := flip(dt[, ttMeaningful])]

dt[, rcAcc        := flip(dt[, rcAcc])]
dt[, rcSat        := flip(dt[, rcSat])]
dt[, rcUseful     := flip(dt[, rcUseful])]
dt[, rcClear      := flip(dt[, rcClear])]
dt[, rcMeaningful := flip(dt[, rcMeaningful])]

dt[, rtFair       := flip(dt[, rtFair])]
dt[, rtAcc        := flip(dt[, rtAcc])]
dt[, rtSat        := flip(dt[, rtSat])]
dt[, rtUseful     := flip(dt[, rtUseful])]
dt[, rtClear      := flip(dt[, rtClear])]
dt[, rtMeaningful := flip(dt[, rtMeaningful])]
```

### Consolidate each metric across treatments
```{r results = 'hide', warning = FALSE}

# Create consolidated data table

dc <- data.table(tFair = rowSums(dt[, c('tcFair', 'ttFair')], na.rm=T))

dc[, complete := !is.na(dt[, random])]
dc[, tAssign := dt[, tTreat] == 1 | dt[, tControl] == 1]
dc[, tControl := !is.na(dt[, tControl])]
dc[, tTreat := !is.na(dt[, tTreat])]
dc[, rControl := !is.na(dt[, rControl])]
dc[, rTreat := !is.na(dt[, rTreat])]
dc[, rAssign := dt[, rTreat] == 1 | dt[, rControl] == 1]
dc[, tweet := !is.na(dt[, tweet])]
dc[, recidivism := !is.na(dt[, recidivism])]

dc[, tAcc := rowSums(dt[, c('tcAcc', 'ttAcc')], na.rm=T)]
dc[, tSat := rowSums(dt[, c('tcSat', 'ttSat')], na.rm=T) ]
dc[, tUseful := rowSums(dt[, c('tcUseful', 'ttUseful')], na.rm=T)]
dc[, tClear := rowSums(dt[, c('tcClear', 'ttClear')], na.rm=T)]
dc[, tMeaningful := rowSums(dt[, c('tcMeaningful', 'ttMeaningful')], na.rm=T)]

dc[, rFair := rowSums(dt[,c('rcFair', 'rtFair')], na.rm=T)]
dc[, rAcc := rowSums(dt[,c('rcAcc', 'rtAcc')], na.rm=T)]
dc[, rSat := rowSums(dt[,c('rcSat', 'rtSat')], na.rm=T) ]
dc[, rUseful := rowSums(dt[,c('rcUseful', 'rtUseful')], na.rm=T)]
dc[, rClear := rowSums(dt[,c('rcClear', 'rtClear')], na.rm=T)]
dc[, rMeaningful := rowSums(dt[,c('rcMeaningful', 'rtMeaningful')], na.rm=T)]

dc[, white := !is.na(dt[, white])]
dc[, black := !is.na(dt[, black])]
dc[, native := !is.na(dt[, native])]
dc[, asian := !is.na(dt[, asian])]
dc[, pac_isle := !is.na(dt[, pac_isle])]
dc[, hispanic := !is.na(dt[, hispanic])]
dc[, other := !is.na(dt[, other])]

dc[, female := (dt[, gender]==2)]
dc[, gender_nc := (dt[, gender]==3)]


dt1 <- dt[, c('ageGroup', 'socMed', 'educ', 'First.Context')]

dc <- cbind(dc,dt1)

# Converting tTreat and rTreat to binaries instead of logicals
(to.replace <- names(which(sapply(dc, is.logical))))
for (var in to.replace) dc[, var:= as.numeric(get(var)), with=FALSE]

#view dc 
head(dc)

```

### Randomization Check

#### Were the two contexts assigned equally?

The first randomization was to assign which context the respondent would see first. Did that work?

```{r}
dc[, .N, by = First.Context]

```
`r dt[First.Context == 'Recidivism', .N]` received the 'Recidivism' context first. `r dt[First.Context == 'Twitter', .N]` received the 'Twitter' context first. This was a pretty even split. Next is worth checking if each context received similar assignment to treatment.

```{r}
dc[, .N, by = .(tTreat, tAssign)]
```
In this instance, we see that of those assigned to the Twitter context (tAssign), it was a pretty even split between treatment and control. However, those 14 that were not assigned to either treatment or control are indicative of attrition that we will need to review in greater detail.
```{r}
dc[, .N, by = .(rTreat, rAssign)]
```
Similarly, we see a pretty even split between recidivicsm context assignment, with another 13 instances of attrition. These could overlap with the other examples of attrition.

From the analysis above, it appears that `r dt[na.omit(tControl), .N]` respondents were assigned to the Twitter-control group, `r dt[na.omit(tTreat), .N]` were assigned to the Twitter-treatment group, `r dt[na.omit(rControl), .N]` were assigned to the recidivism-control group and `r dt[na.omit(rTreat), .N]` were assigned to the recidivism-treatment group.

#### Was treatment assigned equally across contexts? 
```{r}
dc[tAssign == 1, .N, by = .(First.Context, tTreat)]
dc[rAssign ==1, .N, by = .(First.Context, rTreat)]
```
These two tables show that treatment and control were assigned roughly equally across contexts, regardless of order.

#### Were all questions answered?
```{r}
## Show number of responses for each question
apply(dt, 2, function(x) length(which(!is.na(x))))
```
From this, it appears that there were a couple instances of attrition in the middle of answering questions about a treatment. Note the drop from 312 to 311 between ttSat and ttUseful, or the drop from 316 to 315 between rtSat and rtUseful.

#### Attrition effects
Out of `r nrow(dt)` surveys, `r dt[na.omit(random), .N]` were completed. Was either context more impacted than the other?
```{r}
dc[ , sum(complete)/.N, by = First.Context]
```
Similar ratios completed the survey regardless of which context they started with. This does not seem indicative of a problem with the experiment, but we will need to be careful about how we calculate effects.


## Define Metrics
The metrics we evaluated were split into two groups. The first three asked respondents to rate the decision that was made with respect to fairness, accuracy, and their satisfaction with the decision. The second three asked specifically about the explanation itself. Respondents were asked if the explanation was useful, clear, and meaningful.


###Visual data exploration, grouped bars 

###Twitter Response Histograms
```{r}
par(mfrow=c(2,3))
hist(dt$tcFair, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$ttFair,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$tcAcc, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$ttAcc,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcSat, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$ttSat,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcUseful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$ttUseful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcClear, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$ttClear,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcMeaningful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$ttMeaningful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
```

###Recidivism Responses Histogram
```{r}
par(mfrow=c(2,3))
hist(dt$rcFair, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$rtFair,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$rcAcc, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$rtAcc,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcSat, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$rtSat,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcUseful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$rtUseful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcClear, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$rtClear,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcMeaningful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$rtMeaningful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
```

###Twitter Histograms Greyscale
```{r}
par(mfrow=c(2,3))
hist(dt$tcFair, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$ttFair,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$tcAcc, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$ttAcc,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcSat, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$ttSat,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcUseful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$ttUseful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcClear, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$ttClear,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcMeaningful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$ttMeaningful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
```


###Recidivism Histogram Greyscale
```{r}
par(mfrow=c(2,3))
hist(dt$rcFair, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$rtFair,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$rcAcc, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$rtAcc,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcSat, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Satifaction", xlab="Satisfaction", ylim=c(0,175))
#legend(4,9, Treat(df),lwd=4, col=c())
hist(dt$rtSat,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcUseful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$rtUseful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcClear, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$rtClear,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcMeaningful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$rtMeaningful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
```

###Demographic 
```{r}

#ageGroup','race','gender','socMed','educ','feedback','duration'

# par(mfrow=c(2,2))
# hist(dt$gender,breaks=3, main = "Gender")
# hist(dt$socMed,breaks=5, main= "Social Media Usage")
# hist(dt$edu,breaks=5, main = "Educational level")
# hist(dt$duration,breaks=3, main = "Duration")

```



## Regression Models

### Twitter Moderation

Create linear models for each question for both Twitter and recidivism. The models subset the data to look only at those respondents that were assigned to either treatment or control for that context. In this way, someone who attrited in teh first context will not count against the second context.

```{r}
mtFair <- ivreg(tFair ~ tTreat, data = dc[tAssign == 1])
mtAcc <- ivreg(tAcc ~ tTreat, data = dc[tAssign == 1])
mtSat <- ivreg(tSat ~ tTreat, data = dc[tAssign == 1])
mtUseful <- ivreg(tUseful ~ tTreat, data = dc[tAssign == 1])
mtClear <- ivreg(tClear ~ tTreat, data = dc[tAssign == 1])
mtMeaningful <- ivreg(tMeaningful ~ tTreat, data = dc[tAssign == 1])

stargazer(mtFair, mtAcc, mtSat, mtUseful, mtClear, mtMeaningful,
          type = 'text',
          covariate.labels = c("Explanation"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")
```
```{r}
dc[(tAssign == 1 & tSat == 0), .N]
dc[(tAssign == 1 & tMeaningful == 0), .N]

```
This shows that 1 person dropped out between seeing the treatment and responding in the Twitter context. This is probably not affecting our last few metrics, but they are all statistically significant by a large margin anyway. This represents our intent to treat effect.

However, they get through all of the first three questions, so those responses are not affected by attrition.

```{r}
mrFair <- lm(rFair ~ rTreat, data = dc[rAssign == 1])
mrAcc <- lm(rAcc ~ rTreat, data = dc[rAssign == 1])
mrSat <- lm(rSat ~ rTreat, data = dc[rAssign == 1])
mrUseful <- lm(rUseful ~ rTreat, data = dc[rAssign == 1])
mrClear <- lm(rClear ~ rTreat, data = dc[rAssign == 1])
mrMeaningful <- lm(rMeaningful ~ rTreat, data = dc[rAssign == 1])

stargazer(mrFair, mrAcc, mrSat, mrUseful, mrClear, mrMeaningful,
          type = 'text',
          covariate.labels = c("Explanation"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")
```
```{r}
dc[(rAssign == 1 & rSat == 0), .N]
dc[(rAssign == 1 & rMeaningful == 0), .N]

```
This shows that 3 people dropped out between seeing the treatment and responding in the recidivism context. This could be throwing off the last few metrics, but those are all statistically significant by a large margin. This represents our intent to treat effect.

## Difference in Order
We also discussed looking at the difference in responses depending on the order of contexts.

```{r}
otFair <-       lm(tFair ~ First.Context + tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otAcc <-        lm(tAcc ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otSat <-        lm(tSat ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otUseful <-     lm(tUseful ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otClear <-      lm(tClear ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otMeaningful <- lm(tMeaningful ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")
```

In the case of Twitter moderation, there does not seem to be a difference based on order of context.

```{r}
orFair <-       lm(rFair ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orAcc <-        lm(rAcc ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orSat <-        lm(rSat ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orUseful <-     lm(rUseful ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orClear <-      lm(rClear ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orMeaningful <- lm(rMeaningful ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])

library(stargazer)
stargazer(orFair, orAcc, orSat, orUseful, orClear, orMeaningful,
          type = 'text',
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")
```


## Other Factors
```{r}
otFair <-       lm(tFair ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otAcc <-        lm(tAcc ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otSat <-        lm(tSat ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otUseful <-     lm(tUseful ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otClear <-      lm(tClear ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otMeaningful <- lm(tMeaningful ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          covariate.labels = c("Explanation", "Age Group", "Education", "Social Media"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")


```
Women were statistically significantly more likely than men to agree with the Twitter decision. Pacific Islanders did not like the explanation.

```{r}
orFair <-       lm(rFair ~ rTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orAcc <-        lm(rAcc ~ rTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orSat <-        lm(rSat ~ rTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orUseful <-     lm(rUseful ~ rTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orClear <-      lm(rClear ~ rTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orMeaningful <- lm(rMeaningful ~ rTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          covariate.labels = c("Explanation", "Age Group", "Education", "Social Media"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")

```
Again, Pacific Islanders rated the explanation worse than others.

## Data Checks

```{r}
d[is.na(random)]
```



