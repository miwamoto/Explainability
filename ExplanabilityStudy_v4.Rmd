---
title: "Explanability Study"
author: "Michael Amodeo, Krista Mar, Mona Iwamoto"
date: \today
output: pdf_document

---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

# load packages 
library(data.table)
library(foreign)
library(lmtest)
library(sandwich)
library(multiwayvcov)
library(stargazer)
library(AER)
```
## Data Import and Prepration

### Import data
In order to appropriately capture the instances where someone did not finish the survey, whether by clicking through to the final page or by stopping midway, I had to export from Qualtrics in a different format that allows us to see what was viewed and what was not. That also changed some other fields, particularly the multiple choice, multiple selection questions.

```{r}
#Import all data
all_content = readLines("Explainability_Study_legacy_export.csv")

#Delete second and third rows of not useful information
skip_second = all_content[-c(2,3)]

#Create table and data.table
d <-read.csv(textConnection(skip_second), header = TRUE, stringsAsFactors = FALSE)
d <- data.table(d)

remove(all_content, skip_second)

# Create new data table without fields we are not using
dt <- d[,-c('ResponseSet','IPAddress','StartDate','EndDate','RecipientLastName',
          'RecipientFirstName','RecipientEmail','ExternalDataReference','Status',
          'Q_TotalDuration','Enter.Embedded.Data.Field.Name.Here...','LocationLatitude',
          'LocationLongitude','LocationAccuracy', 'Q3.5', 'Q4.5', 'Q6.5', 'Q7.5', 'Q8.1', 
          'Q9.1','Q10.1', 'Q10.3')]

# Rename variables
old_names <- colnames(dt)

##  Key to var names: tc = Twitter control group
#                     tt = Twitter treatment group
#                     rc = recidivism control group 
#                     rt = recidivism treatment group

new_names <- c("ResponseID","Finished","First.Context","random","intro","tweet",
               "tControl", 'tcFair', 'tcAcc', 'tcSat', 'tcUseful', 'tcClear', 
               'tcMeaningful', 'tcReqInfo1', 'tcReqInfo2', 'tcReqInfo3', 'tcReqInfo4', 'tcReqInfo4_txt',
               "tTreat", 'ttFair', 'ttAcc', 'ttSat', 'ttUseful', 'ttClear',
               'ttMeaningful', 'ttReqInfo1', 'ttReqInfo2', 'ttReqInfo3', 'ttReqInfo4', 'ttReqInfo4_txt',
               'recidivism', 
               'rControl', 'rcFair', 'rcAcc', 'rcSat', 'rcUseful', 'rcClear', 
               'rcMeaningful', 'rcReqInfo1', 'rcReqInfo2', 'rcReqInfo3', 'rcReqInfo4', 'rcReqInfo4_txt',
               "rTreat", 'rtFair', 'rtAcc', 'rtSat', 'rtUseful', 'rtClear',
               'rtMeaningful', 'rtReqInfo1', 'rtReqInfo2', 'rtReqInfo3', 'rtReqInfo4',  'rtReqInfo4_txt',
               'ageGroup', 'white', 'black', 'native', 'asian', 'pac_isle', 'hispanic', 
               'other', 'gender', 'socMed', 'educ', 'feedback')

setnames(dt, old_names, new_names)
colnames(dt)
remove(old_names)
```
```{r}
colnames(d)
```


```{r}
#summary(dt)
```

### Data Cleanup
```{r results = 'hide'}
# Most Qualtrics questions were set so the extreme positive value was the first choice (1)
# Function to flip the scale to show more positive as larger number
flip <- function(originalScale) {
  x <- originalScale - 3        #  3 is median
  return(3 - x)
}

# For an unknown reason, question 7.2 Fairness for Recidivism Treatment 
# the values were offset by 24.  This was cross-checked with the text-based responses.

dt$rtFair <- dt$rtFair - 24      # Qualtrics weirdness

# For the Twitter fairness questions, the Qualtrics survey 
# responses were reversed in two instances. All others
# are reversed using the flip function below

# Organize scales so larger values correlate to more fair, more accurate, etc.
dt[, tcAcc        := flip(dt[, tcAcc])]
dt[, tcSat        := flip(dt[, tcSat])]
dt[, tcUseful     := flip(dt[, tcUseful])]
dt[, tcClear      := flip(dt[, tcClear])]
dt[, tcMeaningful := flip(dt[, tcMeaningful])]

dt[, ttFair       := flip(dt[, ttFair])]
dt[, ttAcc        := flip(dt[, ttAcc])]
dt[, ttSat        := flip(dt[, ttSat])]
dt[, ttUseful     := flip(dt[, ttUseful])]
dt[, ttClear      := flip(dt[, ttClear])]
dt[, ttMeaningful := flip(dt[, ttMeaningful])]

dt[, rcAcc        := flip(dt[, rcAcc])]
dt[, rcSat        := flip(dt[, rcSat])]
dt[, rcUseful     := flip(dt[, rcUseful])]
dt[, rcClear      := flip(dt[, rcClear])]
dt[, rcMeaningful := flip(dt[, rcMeaningful])]

dt[, rtFair       := flip(dt[, rtFair])]
dt[, rtAcc        := flip(dt[, rtAcc])]
dt[, rtSat        := flip(dt[, rtSat])]
dt[, rtUseful     := flip(dt[, rtUseful])]
dt[, rtClear      := flip(dt[, rtClear])]
dt[, rtMeaningful := flip(dt[, rtMeaningful])]
```

### Consolidate each metric across treatments
```{r results = 'hide', warning = FALSE}

# Create consolidated data table

dc <- data.table(tFair = rowSums(dt[, c('tcFair', 'ttFair')], na.rm=T))

dc[, complete := !is.na(dt[, random])]
dc[, tAssign := dt[, tTreat] == 1 | dt[, tControl] == 1]
dc[, tControl := !is.na(dt[, tControl])]
dc[, tTreat := !is.na(dt[, tTreat])]
dc[, rControl := !is.na(dt[, rControl])]
dc[, rTreat := !is.na(dt[, rTreat])]
dc[, rAssign := dt[, rTreat] == 1 | dt[, rControl] == 1]
dc[, tweet := !is.na(dt[, tweet])]
dc[, recidivism := !is.na(dt[, recidivism])]

dc[, tAcc := rowSums(dt[, c('tcAcc', 'ttAcc')], na.rm=T)]
dc[, tSat := rowSums(dt[, c('tcSat', 'ttSat')], na.rm=T) ]
dc[, tUseful := rowSums(dt[, c('tcUseful', 'ttUseful')], na.rm=T)]
dc[, tClear := rowSums(dt[, c('tcClear', 'ttClear')], na.rm=T)]
dc[, tMeaningful := rowSums(dt[, c('tcMeaningful', 'ttMeaningful')], na.rm=T)]

dc[, rFair := rowSums(dt[,c('rcFair', 'rtFair')], na.rm=T)]
dc[, rAcc := rowSums(dt[,c('rcAcc', 'rtAcc')], na.rm=T)]
dc[, rSat := rowSums(dt[,c('rcSat', 'rtSat')], na.rm=T) ]
dc[, rUseful := rowSums(dt[,c('rcUseful', 'rtUseful')], na.rm=T)]
dc[, rClear := rowSums(dt[,c('rcClear', 'rtClear')], na.rm=T)]
dc[, rMeaningful := rowSums(dt[,c('rcMeaningful', 'rtMeaningful')], na.rm=T)]

dc[, tReqInfo1 := rowSums(dt[,c('tcReqInfo1', 'ttReqInfo1')], na.rm=T)]
dc[, tReqInfo2 := rowSums(dt[,c('tcReqInfo2', 'ttReqInfo2')], na.rm=T)]
dc[, tReqInfo3 := rowSums(dt[,c('tcReqInfo3', 'ttReqInfo3')], na.rm=T)]
#dc[, tOther4 := rowSums(dt[,c('tcOther4', 'ttOther4')], na.rm=T)]

dc[, rReqInfo1 := rowSums(dt[,c('rcReqInfo1', 'rtReqInfo1')], na.rm=T)]
dc[, rReqInfo2 := rowSums(dt[,c('rcReqInfo2', 'rtReqInfo2')], na.rm=T)]
dc[, rReqInfo3 := rowSums(dt[,c('rcReqInfo3', 'rtReqInfo3')], na.rm=T)]
#dc[, rOther4 := rowSums(dt[,c('rcOther4', 'rtOther4')], na.rm=T)]

dc[, white := !is.na(dt[, white])]
dc[, black := !is.na(dt[, black])]
dc[, native := !is.na(dt[, native])]
dc[, asian := !is.na(dt[, asian])]
dc[, pac_isle := !is.na(dt[, pac_isle])]
dc[, hispanic := !is.na(dt[, hispanic])]
dc[, other := !is.na(dt[, other])]

dc[, female := (dt[, gender]==2)]
dc[, gender_nc := (dt[, gender]==3)]


dt1 <- dt[, c('ageGroup', 'socMed', 'educ', 'First.Context')]

dc <- cbind(dc,dt1)

# Converting tTreat and rTreat to binaries instead of logicals
(to.replace <- names(which(sapply(dc, is.logical))))
for (var in to.replace) dc[, var:= as.numeric(get(var)), with=FALSE]

#view dc 
head(dc)

```

### Randomization Check

#### Were the two contexts assigned equally?

The first randomization was to assign which context the respondent would see first. Did that work?

```{r}
dc[, .N, by = First.Context]

```
`r dt[First.Context == 'Recidivism', .N]` received the 'Recidivism' context first. `r dt[First.Context == 'Twitter', .N]` received the 'Twitter' context first. This was a pretty even split. Next is worth checking if each context received similar assignment to treatment.

```{r}
dc[, .N, by = .(tTreat, tAssign)]
```
In this instance, we see that of those assigned to the Twitter context (tAssign), it was a pretty even split between treatment and control. However, those 14 that were not assigned to either treatment or control are indicative of attrition that we will need to review in greater detail.
```{r}
dc[, .N, by = .(rTreat, rAssign)]
```
Similarly, we see a pretty even split between recidivicsm context assignment, with another 13 instances of attrition. These could overlap with the other examples of attrition.

From the analysis above, it appears that `r dt[na.omit(tControl), .N]` respondents were assigned to the Twitter-control group, `r dt[na.omit(tTreat), .N]` were assigned to the Twitter-treatment group, `r dt[na.omit(rControl), .N]` were assigned to the recidivism-control group and `r dt[na.omit(rTreat), .N]` were assigned to the recidivism-treatment group.

#### Was treatment assigned equally across contexts? 
```{r}
dc[tAssign == 1, .N, by = .(First.Context, tTreat)]
dc[rAssign ==1, .N, by = .(First.Context, rTreat)]
```
These two tables show that treatment and control were assigned roughly equally across contexts, regardless of order.

#### Were all questions answered?
```{r}
## Show number of responses for each question
apply(dt, 2, function(x) length(which(!is.na(x))))
```
From this, it appears that there were a couple instances of attrition in the middle of answering questions about a treatment. Note the drop from 312 to 311 between ttSat and ttUseful, or the drop from 316 to 315 between rtSat and rtUseful.

#### Attrition effects
Out of `r nrow(dt)` surveys, `r dt[na.omit(random), .N]` were completed. Was either context more impacted than the other?
```{r}
dc[ , sum(complete)/.N, by = First.Context]
```
Similar ratios completed the survey regardless of which context they started with. This does not seem indicative of a problem with the experiment, but we will need to be careful about how we calculate effects.


## Define Metrics
The metrics we evaluated were split into two groups. The first three asked respondents to rate the decision that was made with respect to fairness, accuracy, and their satisfaction with the decision. The second three asked specifically about the explanation itself. Respondents were asked if the explanation was useful, clear, and meaningful.


###Visual data exploration, grouped bars 

###Twitter Response Histograms
```{r}
par(mfrow=c(2,3))
hist(dt$tcFair, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$ttFair,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$tcAcc, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$ttAcc,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcSat, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$ttSat,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcUseful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$ttUseful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcClear, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$ttClear,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcMeaningful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$ttMeaningful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
```

###Recidivism Responses Histogram
```{r}
par(mfrow=c(2,3))
hist(dt$rcFair, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$rtFair,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$rcAcc, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$rtAcc,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcSat, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$rtSat,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcUseful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$rtUseful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcClear, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$rtClear,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcMeaningful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), main= "Recidivism Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$rtMeaningful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
```

###Twitter Histograms Greyscale
```{r}
par(mfrow=c(2,3))
hist(dt$tcFair, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$ttFair,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$tcAcc, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$ttAcc,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcSat, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$ttSat,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcUseful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$ttUseful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcClear, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$ttClear,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcMeaningful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Twitter Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$ttMeaningful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
```


###Recidivism Histogram Greyscale
```{r}
par(mfrow=c(2,3))
hist(dt$rcFair, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$rtFair,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$rcAcc, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$rtAcc,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcSat, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Satifaction", xlab="Satisfaction", ylim=c(0,175))
#legend(4,9, Treat(df),lwd=4, col=c())
hist(dt$rtSat,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcUseful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$rtUseful,colx=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcClear, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$rtClear,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcMeaningful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), main= "Recidivism Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$rtMeaningful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
```
###What Additional Information did respondents want

```{r}
tcsumReqInfo1<-sum(dt$tcReqInfo1, na.rm=TRUE)
#tcsumother1
tcsumReqInfo2<-sum(dt$tcReqInfo2, na.rm=TRUE)
#tcsumother2
tcsumReqInfo3<-sum(dt$tcReqInfo3, na.rm=TRUE)
#tcsumother3

ttsumReqInfo1<-sum(dt$ttReqInfo1, na.rm=TRUE)
#ttsumother1
ttsumReqInfo2<-sum(dt$ttReqInfo2, na.rm=TRUE)
#ttsumother2
ttsumReqInfo3<-sum(dt$ttReqInfo3, na.rm=TRUE)
#ttsumother3

rcsumReqInfo1<-sum(dt$rcReqInfo1, na.rm=TRUE)
rcsumReqInfo2<-sum(dt$rcReqInfo2, na.rm=TRUE)
rcsumReqInfo3<-sum(dt$rcReqInfo3, na.rm=TRUE)

rtsumReqInfo1<-sum(dt$rtReqInfo1, na.rm=TRUE)
rtsumReqInfo2<-sum(dt$rtReqInfo2, na.rm=TRUE)
rtsumReqInfo3<-sum(dt$rtReqInfo3, na.rm=TRUE)


Number <- c("Other examples","Relative importance","Algorithm detail")
tc <- c(tcsumReqInfo1,tcsumReqInfo2,tcsumReqInfo3)
tt <- c(ttsumReqInfo1,ttsumReqInfo2,ttsumReqInfo3)
rc <- c(rcsumReqInfo1,rcsumReqInfo2,rcsumReqInfo3)
rt <- c(rtsumReqInfo1,rtsumReqInfo2,rtsumReqInfo3)
nyx <- data.frame(Number,tc,tt, rc, rt)

# load needed libraries
library(reshape2)
library(ggplot2)

# reshape your data into long format
nyxlong <- melt(nyx, id=c("Number"))


# make the plot
ggplot(nyxlong) +
  geom_bar(aes(x = Number, y = value, fill = variable),
           stat="identity", position = "dodge", width = 0.7) +
  scale_fill_manual("Result\n", values = c("deepskyblue","blue4", "firebrick1","firebrick4"),
                    labels = c("Twitter control", "Twitter treatment","Recidivism control", "Recidivism treatment")) +
  labs(x="\nAdditional Explanation",y="Result\n") +
  theme_bw(base_size = 14)

```






###Not to include in report, but text outputs of Other q 4 
##Twitter control 
again, whether there's any oversight into the decision or any appeals (though in this specific case it was clearly valid)
Explanation of why they feel it's right to limit free speech.
Freedom of speech infringement, you can not like the guy but twitter shouldn't silence his viewpoint, individuals should block him. When you are a platform for social interaction you shouldn't be allowed to restrict who can say what.
None, I am against the restriction of the freedom of speech.
none, moderation should be done by humans
Statistics on pattern of behavior of banned individual, whether wrong people have ever been flagged
What conduct rule in particular was deemed violated by the algorithm.

##Twitter treatment
A contextual analysis of the actual subject of the comment
an explanation of the 70% threshold
Definition of the characteristics
Explanation of Sentiment Analysis
Explanation on what is sentiment analysis and how it was judged
I am perfectly satisfied with the explanation exactly as it is.
I'm satisfied with the information.
It should be stacked. All of the bars should add together. The way it's set up now, it could be at 69% threshold for all criteria and still would not have any action taken against it.
more detailed explanation of the graph
No other information required.
nothing, it is ridiculous
Proper spelling and explanation of sentiment decisions
What "sentiment analysis" means. Is this thing reacting to everything it views as expressing a negative or argumentative attitude?
when does using offensive vocabulary mean you are guilty, then pretty much all of us would be in jail and not got out of college
Why Twitter feels the need to implement an algorithm like this at all.

##Recidivism control
A human isn't simple enough, there are going to be many factors that won't be considered.
Again - riduculous just as previous answer - doesn't take "person" into account, just numbers
An explanation of why and how they use an algorithm in court--and who allowed it.
basically, it needs tons more information to make a decision like that
none
past success percentage
Statistics on accuracy
the specific information the algorithm uses to make the assessment
what happened to innocent until proven guilty?  you can not make assessment of someone based on algorithm when they did not do anything wrong
whether there's any human oversight into the decision

##Recidivism treatment
 A possibility of a human psychologist or social worker weighing in on the algorithm results too.
An explanation of how level of criminal personality was determined.
biases of those who wrote the algorithm
Definitions of each
How it can assume that everyone is the same based on answers.
human evaluation
I want to review the source code, the questions, the regression test results (when it was tested on repeat offenders)
I would like to know what information the algorithm considered when making the decision.
I'm already satisfied with the explanation.
More detailed breakdown of what is meant in this context by phrases like "criminal personality"
NONE
Numbers
records showing other uses that turned out to be accurate and the percentage of accuracy overall



###Demographic 
```{r}
#ageGroup','race','gender','socMed','educ','feedback','duration'
# par(mfrow=c(2,2))
# hist(dt$gender,  main = "Gender")
# hist(dt$ageGroup, main = "Age")
# hist(dt$socMed, main= "Social Media Usage")
# hist(dt$edu,  main = "Educational level")
#barplot(prop.table(table(dt$race)))

```

Interesting? Our MTers are mostly college students?



## Regression Models

### Twitter Moderation

Create linear models for each question for both Twitter and recidivism. The models subset the data to look only at those respondents that were assigned to either treatment or control for that context. In this way, someone who attrited in teh first context will not count against the second context.

```{r}
mtFair <- ivreg(tFair ~ tTreat, data = dc[tAssign == 1])
mtAcc <- ivreg(tAcc ~ tTreat, data = dc[tAssign == 1])
mtSat <- ivreg(tSat ~ tTreat, data = dc[tAssign == 1])
mtUseful <- ivreg(tUseful ~ tTreat, data = dc[tAssign == 1])
mtClear <- ivreg(tClear ~ tTreat, data = dc[tAssign == 1])
mtMeaningful <- ivreg(tMeaningful ~ tTreat, data = dc[tAssign == 1])

stargazer(mtFair, mtAcc, mtSat, mtUseful, mtClear, mtMeaningful,
          type = 'text',
          covariate.labels = c("Explanation"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")
```
```{r}
dc[(tAssign == 1 & tSat == 0), .N]
dc[(tAssign == 1 & tMeaningful == 0), .N]

```
This shows that 1 person dropped out between seeing the treatment and responding in the Twitter context. This is probably not affecting our last few metrics, but they are all statistically significant by a large margin anyway. This represents our intent to treat effect.

However, they get through all of the first three questions, so those responses are not affected by attrition.

```{r}
mrFair <- lm(rFair ~ rTreat, data = dc[rAssign == 1])
mrAcc <- lm(rAcc ~ rTreat, data = dc[rAssign == 1])
mrSat <- lm(rSat ~ rTreat, data = dc[rAssign == 1])
mrUseful <- lm(rUseful ~ rTreat, data = dc[rAssign == 1])
mrClear <- lm(rClear ~ rTreat, data = dc[rAssign == 1])
mrMeaningful <- lm(rMeaningful ~ rTreat, data = dc[rAssign == 1])

stargazer(mrFair, mrAcc, mrSat, mrUseful, mrClear, mrMeaningful,
          type = 'text',
          covariate.labels = c("Explanation"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")
```
```{r}
dc[(rAssign == 1 & rSat == 0), .N]
dc[(rAssign == 1 & rMeaningful == 0), .N]

```
This shows that 3 people dropped out between seeing the treatment and responding in the recidivism context. This could be throwing off the last few metrics, but those are all statistically significant by a large margin. This represents our intent to treat effect.

## Difference in Order
We also discussed looking at the difference in responses depending on the order of contexts.

```{r}
otFair <-       lm(tFair ~ First.Context + tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otAcc <-        lm(tAcc ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otSat <-        lm(tSat ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otUseful <-     lm(tUseful ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otClear <-      lm(tClear ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])
otMeaningful <- lm(tMeaningful ~ First.Context+ tTreat + rTreat + tTreat*rTreat, data = dc[tAssign == 1])

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")
```

In the case of Twitter moderation, there does not seem to be a difference based on order of context.

```{r}
orFair <-       lm(rFair ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orAcc <-        lm(rAcc ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orSat <-        lm(rSat ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orUseful <-     lm(rUseful ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orClear <-      lm(rClear ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])
orMeaningful <- lm(rMeaningful ~ First.Context + rTreat + tTreat + rTreat*tTreat, data = dc[rAssign == 1])

library(stargazer)
stargazer(orFair, orAcc, orSat, orUseful, orClear, orMeaningful,
          type = 'text',
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")
```


## Other Factors

```{r}
otFair <-       lm(tFair ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otAcc <-        lm(tAcc ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otSat <-        lm(tSat ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otUseful <-     lm(tUseful ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otClear <-      lm(tClear ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
otMeaningful <- lm(tMeaningful ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          covariate.labels = c("Explanation", "Age Group", "Education", "Social Media"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")


```
Women were statistically significantly more likely than men to agree with the Twitter decision. Pacific Islanders did not like the explanation.

```{r}
orFair <-       lm(rFair ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orAcc <-        lm(rAcc ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orSat <-        lm(rSat ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orUseful <-     lm(rUseful ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orClear <-      lm(rClear ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)
orMeaningful <- lm(rMeaningful ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          covariate.labels = c("Explanation", "Age Group", "Education", "Social Media", "Social Media2"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")

```
Again, Pacific Islanders rated the explanation worse than others.

## Data Checks

```{r}
d[is.na(random)]
```



