---
title: "Explanability Study"
author: "Michael Amodeo, Krista Mar, Mona Iwamoto"
date: \today
output: pdf_document

---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = TRUE)

# load packages 
library(data.table)
library(foreign)
library(lmtest)
library(sandwich)
library(multiwayvcov)
library(stargazer)
library(AER)
library(reshape2)
library(ggplot2)
```
## Data Import and Prepration

### Import data
Data was exported from Qualtrics using the Legacy Format CSV export. This allowed for additional fields based on whether questions were seen, even if the questions did not require answers.

```{r}
#Import all data
all_content = readLines("Explainability_Study_legacy_export.csv")

#Delete second and third rows of not useful information
skip_second = all_content[-c(2,3)]

#Create table and data.table
d <-read.csv(textConnection(skip_second), header = TRUE, stringsAsFactors = FALSE)
d <- data.table(d)

remove(all_content, skip_second)

# Create new data table without fields we are not using
dt <- d[,-c('ResponseSet','IPAddress','StartDate','EndDate','RecipientLastName',
          'RecipientFirstName','RecipientEmail','ExternalDataReference','Status',
          'Q_TotalDuration','Enter.Embedded.Data.Field.Name.Here...','LocationLatitude',
          'LocationLongitude','LocationAccuracy', 'Q3.5', 'Q4.5', 'Q6.5', 'Q7.5', 'Q8.1', 
          'Q9.1','Q10.1', 'Q10.3')]

# Rename variables
old_names <- colnames(dt)

##  Key to var names: tc = Twitter control group
#                     tt = Twitter treatment group
#                     rc = recidivism control group 
#                     rt = recidivism treatment group

new_names <- c("ResponseID","Finished","First.Context","random","intro","tweet",
               "tControl", 'tcFair', 'tcAcc', 'tcSat', 'tcUseful', 'tcClear', 
               'tcMeaningful', 'tcReqInfo1', 'tcReqInfo2', 'tcReqInfo3', 'tcReqInfo4',
               'tcReqInfo4_txt',
               "tTreat", 'ttFair', 'ttAcc', 'ttSat', 'ttUseful', 'ttClear',
               'ttMeaningful', 'ttReqInfo1', 'ttReqInfo2', 'ttReqInfo3', 'ttReqInfo4',
               'ttReqInfo4_txt',
               'recidivism', 
               'rControl', 'rcFair', 'rcAcc', 'rcSat', 'rcUseful', 'rcClear', 
               'rcMeaningful', 'rcReqInfo1', 'rcReqInfo2', 'rcReqInfo3', 'rcReqInfo4',
               'rcReqInfo4_txt',
               "rTreat", 'rtFair', 'rtAcc', 'rtSat', 'rtUseful', 'rtClear',
               'rtMeaningful', 'rtReqInfo1', 'rtReqInfo2', 'rtReqInfo3', 'rtReqInfo4',
               'rtReqInfo4_txt',
               'ageGroup', 'white', 'black', 'native', 'asian', 'pac_isle', 'hispanic', 
               'other', 'gender', 'socMed', 'educ', 'feedback')

setnames(dt, old_names, new_names)
colnames(dt)
remove(old_names)
```


### Data Cleanup

The questions were based on a 5 point Likert scale. For each metric, the answers varied from "Extremely" to "Not at All." Most Qualtrics questions were set so the extreme positive value was the first choice (1). In order to show an increase in acceptance or trust as positive, we will rescale these values and flip them around the median value (3). 

```{r results = 'hide'}
# Function to flip the scale to show more positive as larger number
flip <- function(originalScale) {
  x <- originalScale - 3        #  3 is median
  return(3 - x)
}

# For an unknown reason, question 7.2 Fairness for Recidivism Treatment 
# the values were offset by 24.  This was cross-checked with the text-based responses.

dt$rtFair <- dt$rtFair - 24      # Qualtrics weirdness
dt$educ <- dt$educ - 10      # Qualtrics weirdness


# For the Twitter fairness questions, the Qualtrics survey 
# responses were reversed in two instances. All others
# are reversed using the flip function below

# Organize scales so larger values correlate to more fair, more accurate, etc.

flip_cols <- c("tcAcc", "tcSat", "tcUseful", "tcClear", "tcMeaningful", 
               "ttFair", "ttAcc", "ttSat", "ttUseful", "ttClear", "ttMeaningful", 
               "rcAcc", "rcSat", "rcUseful", "rcClear", "rcMeaningful", 
               "rtFair", "rtAcc", "rtSat", "rtUseful", "rtClear", "rtMeaningful")

dt[,  (flip_cols) := lapply(.SD, flip), .SDcols = flip_cols]
```

### Consolidate each metric across treatments

Because the Qualtrics format requires each question to be different, we have to consolidate the responses for each metric into a single value per context. Because respondents either saw control or treatment for each context, we simply make a new metric that is the sum of the old metrics.

```{r results = 'hide', warning = FALSE}

# Create consolidated data table

dc <- data.table(ResponseID = dt[, ResponseID])

dc[, complete := !is.na(dt[, random])] # Did they complete the survey?
dc[, tAssign := dt[, tTreat] == 1 | dt[, tControl] == 1] #Was a Twitter treatment assigned?
dc[, tControl := !is.na(dt[, tControl])]
dc[, tTreat := !is.na(dt[, tTreat])]
dc[, rControl := !is.na(dt[, rControl])]
dc[, rTreat := !is.na(dt[, rTreat])]
dc[, rAssign := dt[, rTreat] == 1 | dt[, rControl] == 1] #Was a recidivism treatment assigned?
dc[, tweet := !is.na(dt[, tweet])] # Did they reach the Twitter context?
dc[, recidivism := !is.na(dt[, recidivism])] # Did they reach the recidivism context?

dc[, tFair := rowSums(dt[, c('tcFair', 'ttFair')], na.rm=T)]
dc[, tAcc := rowSums(dt[, c('tcAcc', 'ttAcc')], na.rm=T)]
dc[, tSat := rowSums(dt[, c('tcSat', 'ttSat')], na.rm=T) ]
dc[, tUseful := rowSums(dt[, c('tcUseful', 'ttUseful')], na.rm=T)]
dc[, tClear := rowSums(dt[, c('tcClear', 'ttClear')], na.rm=T)]
dc[, tMeaningful := rowSums(dt[, c('tcMeaningful', 'ttMeaningful')], na.rm=T)]

dc[, rFair := rowSums(dt[,c('rcFair', 'rtFair')], na.rm=T)]
dc[, rAcc := rowSums(dt[,c('rcAcc', 'rtAcc')], na.rm=T)]
dc[, rSat := rowSums(dt[,c('rcSat', 'rtSat')], na.rm=T) ]
dc[, rUseful := rowSums(dt[,c('rcUseful', 'rtUseful')], na.rm=T)]
dc[, rClear := rowSums(dt[,c('rcClear', 'rtClear')], na.rm=T)]
dc[, rMeaningful := rowSums(dt[,c('rcMeaningful', 'rtMeaningful')], na.rm=T)]

dc[, tReqInfo1 := rowSums(dt[,c('tcReqInfo1', 'ttReqInfo1')], na.rm=T)]
dc[, tReqInfo2 := rowSums(dt[,c('tcReqInfo2', 'ttReqInfo2')], na.rm=T)]
dc[, tReqInfo3 := rowSums(dt[,c('tcReqInfo3', 'ttReqInfo3')], na.rm=T)]

dc[, rReqInfo1 := rowSums(dt[,c('rcReqInfo1', 'rtReqInfo1')], na.rm=T)]
dc[, rReqInfo2 := rowSums(dt[,c('rcReqInfo2', 'rtReqInfo2')], na.rm=T)]
dc[, rReqInfo3 := rowSums(dt[,c('rcReqInfo3', 'rtReqInfo3')], na.rm=T)]

dc[, white := !is.na(dt[, white])]
dc[, black := !is.na(dt[, black])]
dc[, native := !is.na(dt[, native])]
dc[, asian := !is.na(dt[, asian])]
dc[, pac_isle := !is.na(dt[, pac_isle])]
dc[, hispanic := !is.na(dt[, hispanic])]
dc[, other := !is.na(dt[, other])]

dc[, female := (dt[, gender]==2)]
dc[, gender_nc := (dt[, gender]==3)]


dt1 <- dt[, c('ageGroup', 'socMed', 'educ', 'First.Context')]

dc <- cbind(dc,dt1)

# Converting tTreat and rTreat to binaries instead of logicals
(to.replace <- names(which(sapply(dc, is.logical))))
for (var in to.replace) dc[, var:= as.numeric(get(var)), with=FALSE]

head(dc)

```

### Randomization Check

#### Were the two contexts assigned equally?

The first randomization assigned which context the respondent would see first. Check to see if that rnadomization evenly distributed the order of contexts seen.

```{r}
dc[, .N, by = First.Context]

```

`r dt[First.Context == 'Recidivism', .N]` received the 'Recidivism' context first. `r dt[First.Context == 'Twitter', .N]` received the 'Twitter' context first. This was a pretty even split. Next we check if each context received similar assignment to treatment.

```{r}
dc[, .N, by = .(tTreat, tAssign)]
```

In this instance, we see that of those assigned to either treatment or control in the Twitter context (tAssign), it was a pretty even split between treatment and control. However, those 14 that were not assigned to either treatment or control are indicative of attrition that we will need to review in greater detail.

```{r}
dc[, .N, by = .(rTreat, rAssign)]
```

Similarly, we see a pretty even split between recidivism context assignment, with another 13 instances of attrition. These could overlap with the other examples of attrition.

#### Was treatment assigned equally across contexts? 
```{r}
dc[complete == 1, .N, by = .(First.Context, tTreat, rTreat)]
```

Of the eight possible combinations of context order, Twitter treatment, and recidivism treatment, there are a fairly equal number of respondents who completed the survey in each category. This shows that our randomization worked at every level.


#### Were all questions answered?
```{r}
## Show number of responses for each question
apply(dt, 2, function(x) length(which(!is.na(x))))
```
From this, it appears that there were a couple instances of attrition in the middle of answering questions about a treatment. Note the drop from 312 to 311 between ttSat and ttUseful, or the drop from 316 to 315 between rtSat and rtUseful.

#### Attrition effects
Out of `r nrow(dt)` surveys, `r dt[na.omit(random), .N]` were completed. Was either context more impacted than the other?
```{r}
dc[ , sum(complete)/.N, by = First.Context]
```
Similar ratios completed the survey regardless of which context they started with. This does not seem indicative of a problem with the experiment, but we will need to be careful about how we calculate effects.

```{r}
dc[, .N, by = .(First.Context, tTreat, rTreat, tAssign, rAssign)]
```

To look again at all possible combinations, we see that the largest number of dropouts we had were the 10 who were assigned a context but did not make it far enough to be assigned to treatment or control for either context. These respondents must have followed the link to Qualtrics but then dropped out without doing anything within Qualtrics. The other instances of attrition are pretty small and even (1 or 2 for each group who did not make it to a second context). Overall, there might be a very small effect because of attrition, but it does not seem to be due to the experiment design or content and does not affect the contexts differently.

## Define Metrics

The metrics we evaluated were split into two groups. The first three asked respondents to rate the decision that was made with respect to fairness, accuracy, and their satisfaction with the decision. The second three asked specifically about the explanation itself. Respondents were asked if the explanation was useful, clear, and meaningful. Again, each of these responses were based on a 5 point Likert scale.

###Visual data exploration, grouped bars 

###Twitter Response Histograms
```{r}
par(mfrow=c(2,3))
hist(dt$tcFair, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$ttFair,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$tcAcc, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$ttAcc,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcSat, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$ttSat,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcUseful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$ttUseful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcClear, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$ttClear,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcMeaningful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$ttMeaningful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
```

###Recidivism Responses Histogram
```{r}
par(mfrow=c(2,3))
hist(dt$rcFair, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$rtFair,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$rcAcc, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$rtAcc,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcSat, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$rtSat,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcUseful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$rtUseful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcClear, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$rtClear,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcMeaningful, col=rgb(0,0,1,1/4), breaks = seq(0.5, 5.5, 1),
     main= "Recidivism Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$rtMeaningful,col=rgb(1,0,0,1/4), breaks = seq(0.5, 5.5, 1), add=T)
```

In both contexts, we can see a difference between control and treatment that increases each of the metrics under treatment.

###Twitter Histograms Greyscale
```{r}
par(mfrow=c(2,3))
hist(dt$tcFair, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$ttFair,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$tcAcc, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$ttAcc,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcSat, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Satifaction", xlab="Satisfaction", ylim=c(0,175))
hist(dt$ttSat,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcUseful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$ttUseful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcClear, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$ttClear,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$tcMeaningful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Twitter Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$ttMeaningful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
```


###Recidivism Histogram Greyscale
```{r}
par(mfrow=c(2,3))
hist(dt$rcFair, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Fairness", xlab="Fairness", ylim=c(0,175))
hist(dt$rtFair,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
#legend("topright", c("Control", "Treatment"), fill=c("blue", "red"))

hist(dt$rcAcc, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Accuracy", xlab="Accuracy", ylim=c(0,175))
hist(dt$rtAcc,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcSat, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Satifaction", xlab="Satisfaction", ylim=c(0,175))
#legend(4,9, Treat(df),lwd=4, col=c())
hist(dt$rtSat,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcUseful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Usefulness", xlab="Usefulness", ylim=c(0,175))
hist(dt$rtUseful,colx=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcClear, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Clarity", xlab="Clarity", ylim=c(0,175))
hist(dt$rtClear,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)

hist(dt$rcMeaningful, col=rgb(0.1,0.1,0.1,0.5), breaks = seq(0.5, 5.5, 1), 
     main= "Recidivism Meaningfulness", xlab="Meaningfulness", ylim=c(0,175))
hist(dt$rtMeaningful,col=rgb(0.8,0.8,0.8,0.5), breaks = seq(0.5, 5.5, 1), add=T)
```



###Demographic Data Review
```{r}
#ageGroup','race','gender','socMed','educ','feedback','duration'
# par(mfrow=c(2,2))

hist(dt$gender,  main = "Gender", breaks = seq(0.5, 3.5, 1))
hist(dt$ageGroup*10, main = "Age", breaks = seq(5, 85, 10))
hist(dt$socMed, main= "Social Media Usage", breaks = seq(0.5, 5.5, 1))
hist(dt$educ, main = "Educational Level", breaks = seq(0.5, 7.5, 1))

ethnic <- c("White", "African American", "Asian", "Hispanic", "Pacific Islander", "Other")
ethnicities <- data.table(sum(!is.na(dt$white)), sum(!is.na(dt$black)), sum(!is.na(dt$asian)), 
                          sum(!is.na(dt$hispanic)), sum(!is.na(dt$pac_isle)), sum(!is.na(dt$other)))
ethnicities2 = transpose(ethnicities)
barplot(ethnicities2$V1, names = ethnic)
```

Based on a quick look at our survey demographic responses, we see that approximately 2/3 of the respondents are male. The respondents also skew young, as almost half are between 25 and 34. Nearly 500 of our 641 respondents use social media daily, which may be biasing our results. More than half have completed a 4 year degree or higher. The respondents are also over 2/3 white.

## Regression Models

A basic view of the data showed there was a change in the distributions. Regression models will allow us to gauge the significance of these changes. We have create linear models for each question for each context (Twitter and recidivism). The models subset the data to look only at those respondents that were assigned to either treatment or control for that context. In this way, someone who attrited in the first context will not count against the second context.

### Twitter Moderation

```{r}
mtFair <- ivreg(tFair ~ tTreat, data = dc[tAssign == 1])
mtAcc <- ivreg(tAcc ~ tTreat, data = dc[tAssign == 1])
mtSat <- ivreg(tSat ~ tTreat, data = dc[tAssign == 1])
mtUseful <- ivreg(tUseful ~ tTreat, data = dc[tAssign == 1])
mtClear <- ivreg(tClear ~ tTreat, data = dc[tAssign == 1])
mtMeaningful <- ivreg(tMeaningful ~ tTreat, data = dc[tAssign == 1])

stargazer(mtFair, mtAcc, mtSat, mtUseful, mtClear, mtMeaningful,
          type = 'text',
          covariate.labels = c("Explanation"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")
```
```{r}
dc[(tAssign == 1 & tSat == 0), .N]
dc[(tAssign == 1 & tMeaningful == 0), .N]

```
This shows that 1 person dropped out between seeing the treatment and responding in the Twitter context. This is probably not affecting our last few metrics, but they are all statistically significant by a large margin anyway. This represents our intent to treat effect for the questions they did not answer. However, they get through all of the first three questions, so those responses are not affected by attrition.

```{r}
mrFair <- lm(rFair ~ rTreat, data = dc[rAssign == 1])
mrAcc <- lm(rAcc ~ rTreat, data = dc[rAssign == 1])
mrSat <- lm(rSat ~ rTreat, data = dc[rAssign == 1])
mrUseful <- lm(rUseful ~ rTreat, data = dc[rAssign == 1])
mrClear <- lm(rClear ~ rTreat, data = dc[rAssign == 1])
mrMeaningful <- lm(rMeaningful ~ rTreat, data = dc[rAssign == 1])

stargazer(mrFair, mrAcc, mrSat, mrUseful, mrClear, mrMeaningful,
          type = 'text',
          covariate.labels = c("Explanation"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")
```
```{r}
dc[(rAssign == 1 & rSat == 0), .N]
dc[(rAssign == 1 & rMeaningful == 0), .N]

```
This shows that 3 people dropped out between seeing the treatment and responding in the recidivism context. This could be throwing off the last few metrics, but those are all statistically significant by a large margin. This represents our intent to treat effect.

## Comparison of Contexts

Our second hypothesis asked if there was a difference between how respondents evaluated the explanation in two different contexts of varying importance or personal significance.

```{r results = 'hide'} 
dc2 <- melt(dc, id.vars = c('ResponseID', 'tAssign', 'tControl', 'rAssign',
                            'rControl', "tweet", "recidivism", "tFair", "tAcc",
                            "tSat", "tUseful", "tClear", "tMeaningful", "rFair",
                            "rAcc", "rSat", "rUseful", "rClear", "rMeaningful"),
            measure.vars = c('tTreat', 'rTreat'))

dc2[, Fair := (variable == 'tTreat')*tFair + (variable == 'rTreat')*rFair]
dc2[, Acc := (variable == 'tTreat')*tAcc + (variable == 'rTreat')*rAcc]
dc2[, Sat := (variable == 'tTreat')*tSat + (variable == 'rTreat')*rSat]
dc2[, Useful := (variable == 'tTreat')*tUseful + (variable == 'rTreat')*rUseful]
dc2[, Clear := (variable == 'tTreat')*tClear + (variable == 'rTreat')*rClear]
dc2[, Meaningful := (variable == 'tTreat')*tMeaningful + (variable == 'rTreat')*rMeaningful]
names(dc2)[names(dc2) == "variable"] = "Context"
names(dc2)[names(dc2) == "value"] = "treat"

dc2[,c("tFair", "tAcc", "tSat", "tUseful", "tClear", "tMeaningful", "rFair", "rAcc", "rSat", 
       "rUseful", "rClear", "rMeaningful"):=NULL]
```

```{r}
mFair <- lm(Fair ~ factor(Context) + treat + treat*factor(Context), 
            data = dc2[rAssign == 1 & tAssign == 1])
mAcc <- lm(Acc ~ factor(Context) + treat + treat*factor(Context), 
            data = dc2[rAssign == 1 & tAssign == 1])
mSat <- lm(Sat ~ factor(Context) + treat + treat*factor(Context), 
            data = dc2[rAssign == 1 & tAssign == 1])
mClear <- lm(Clear ~ factor(Context) + treat + treat*factor(Context), 
            data = dc2[rAssign == 1 & tAssign == 1])
mUseful <- lm(Useful ~ factor(Context) + treat + treat*factor(Context), 
            data = dc2[rAssign == 1 & tAssign == 1])
mMeaningful <- lm(Meaningful ~ factor(Context) + treat + treat*factor(Context), 
            data = dc2[rAssign == 1 & tAssign == 1])
stargazer(mFair, mAcc, mSat, mClear, mUseful, mMeaningful, type = 'text',
          covariate.labels = c("Recidivism Context", "Treatment", "Recidivism Treatment"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = c("Context Comparison"))
```

When comparing contexts, we see statistical significance in the baseline contant for all metrics in the fourth row. This represents the constant in the Twitter control group. In the first row of the regression, we see the change to recidivism has a significant negative effect in all metrics. This means that respondents were less accepting of the algorithm's decision without an explanation than they were in the Twitter context. This may be partly attributable to the design of the survey. Criminal recidivism is a complicated problem with more inputs than a 140 character Tweet. Because we included the full Tweet, people were able to judge the appropriateness of the decision by themselves. In the recidivism context, respondents were only given a brief description of the case, with just the offense the defendant was being charged with. Including some information about criminal history or other factors may have made this a more appropriate comparison.

In the second row, we see what is effectively the same significance of the Twitter treatment that we saw in the Twitter only models. The numbers are slightly different here because this analysis looks only at individuals who were assigned to treatment or control in both contexts. So a a few instances are missing where an individual did not make it to the second half of their survey. In the third row, we see the effect of the recidivism treatment compared to the effect of the Twitter treatment. Again, we have high statistical significance in all metrics as we did in the recidivism only models. However, the significance of the differences is less than of the treatment itself, dropping in 4 of the 6 cases below the 0.01 level, although still significant at a level of 0.05.

## Difference in Order
We also discussed looking at the difference in responses depending on the order of contexts. Significant effects here would show whether answering one context first created a bias in the response to the second context.

```{r}
otFair <-       lm(tFair ~ First.Context + tTreat + rTreat + tTreat*rTreat, 
                   data = dc[tAssign == 1])
otAcc <-        lm(tAcc ~ First.Context+ tTreat + rTreat + tTreat*rTreat, 
                   data = dc[tAssign == 1])
otSat <-        lm(tSat ~ First.Context+ tTreat + rTreat + tTreat*rTreat, 
                   data = dc[tAssign == 1])
otUseful <-     lm(tUseful ~ First.Context+ tTreat + rTreat + tTreat*rTreat, 
                   data = dc[tAssign == 1])
otClear <-      lm(tClear ~ First.Context+ tTreat + rTreat + tTreat*rTreat,
                   data = dc[tAssign == 1])
otMeaningful <- lm(tMeaningful ~ First.Context+ tTreat + rTreat + tTreat*rTreat, 
                   data = dc[tAssign == 1])

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          covariate.labels = c("Twitter First", "Twitter Treatment", 
                               "Recidivism Treatment", "Both Treatments" ),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")
```

In the case of Twitter moderation, there does not seem to be a difference based on order of context. There is significance to receiving the Twitter treatment, but there is no significance to receiving the Twitter context first. There is also not significance to whether or not the recidivism treatment was received.

```{r}
orFair <-       lm(rFair ~ First.Context + rTreat + tTreat + rTreat*tTreat, 
                   data = dc[rAssign == 1])
orAcc <-        lm(rAcc ~ First.Context + rTreat + tTreat + rTreat*tTreat, 
                   data = dc[rAssign == 1])
orSat <-        lm(rSat ~ First.Context + rTreat + tTreat + rTreat*tTreat, 
                   data = dc[rAssign == 1])
orUseful <-     lm(rUseful ~ First.Context + rTreat + tTreat + rTreat*tTreat, 
                   data = dc[rAssign == 1])
orClear <-      lm(rClear ~ First.Context + rTreat + tTreat + rTreat*tTreat, 
                   data = dc[rAssign == 1])
orMeaningful <- lm(rMeaningful ~ First.Context + rTreat + tTreat + rTreat*tTreat, 
                   data = dc[rAssign == 1])

library(stargazer)
stargazer(orFair, orAcc, orSat, orUseful, orClear, orMeaningful,
          type = 'text',
          covariate.labels = c("Twitter First", "Recidivism Treatment", "Twitter Treatment", 
                               "Both Treatments" ),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")
```
In most cases, we see that the only statistical significance is the base rating and the effect of the recidivism treatment. In two cases (Clarity and Usefulness), there is a significant decrease in the metric if Twitter was viewed first. This is perhaps concerning, but the fact that it is negative shows that the actual effect of the recidivism treatment was more positive than we had previously shown. 

## Influence of Other Factors - Demographics, etc

```{r}
otFair <-       lm(tFair ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic 
                   + other + pac_isle + female + gender_nc, data = dc)
otAcc <-        lm(tAcc ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic 
                   + other + pac_isle + female + gender_nc, data = dc)
otSat <-        lm(tSat ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic 
                   + other + pac_isle + female + gender_nc, data = dc)
otUseful <-     lm(tUseful ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic 
                   + other + pac_isle + female + gender_nc, data = dc)
otClear <-      lm(tClear ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic 
                   + other + pac_isle + female + gender_nc, data = dc)
otMeaningful <- lm(tMeaningful ~ tTreat + ageGroup + educ + socMed + black + asian + hispanic 
                   + other + pac_isle + female + gender_nc, data = dc)

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          covariate.labels = c("Explanation", "Age Group", "Education", "Social Media"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Twitter Moderation")


```
Women were statistically significantly more likely than men to agree with the Twitter decision. Pacific Islanders did not like the explanation. However, there were very few Pacific Islanders who responded, and the significance was not strong. Also, with so many metrics and variables, it is highly likely to see significance somewhere at a 0.05 level.

```{r}
orFair <-       lm(rFair ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian 
                   + hispanic + other + pac_isle + female + gender_nc, data = dc)
orAcc <-        lm(rAcc ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian 
                   + hispanic + other + pac_isle + female + gender_nc, data = dc)
orSat <-        lm(rSat ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian 
                   + hispanic + other + pac_isle + female + gender_nc, data = dc)
orUseful <-     lm(rUseful ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian 
                   + hispanic + other + pac_isle + female + gender_nc, data = dc)
orClear <-      lm(rClear ~ rTreat + ageGroup + educ + socMed + socMed^2 + black + asian 
                   + hispanic + other + pac_isle + female + gender_nc, data = dc)
orMeaningful <- lm(rMeaningful ~ rTreat + ageGroup + educ + socMed + socMed^2 + black 
                   + asian + hispanic + other + pac_isle + female + gender_nc, data = dc)

library(stargazer)
stargazer(otFair, otAcc, otSat, otUseful, otClear, otMeaningful,
          type = 'text',
          covariate.labels = c("Explanation", "Age Group", "Education", "Social Media", 
                               "Social Media2"),
          dep.var.labels = c("Fairness", "Accuracy", "Satisfaction", "Usefulness",
                             "Clarity", "Meaningfulness"),
          dep.var.caption = "Recidivism Risk Assessment")

```
Again, Pacific Islanders rated the explanation worse than others. Women were more likely to agree with the decision than men. There were also other significant effects throughout the table, but they do not fit a pattern across metrics.



###What Additional Information did respondents want

In each context, respondents were asked what information they would like to see as part of an explanation. This question was multiple choice with multiple selection allowed as well as a text write-in section. The three options were:

a. Examples of other levels of decision output
b. Relative importance of the characteristics that led to the decision
c. Detailed description of how the algorithm works.

```{r}
# Find total number of those who selected each option.
tcsumReqInfo1<-sum(dt$tcReqInfo1, na.rm=TRUE)
tcsumReqInfo2<-sum(dt$tcReqInfo2, na.rm=TRUE)
tcsumReqInfo3<-sum(dt$tcReqInfo3, na.rm=TRUE)

ttsumReqInfo1<-sum(dt$ttReqInfo1, na.rm=TRUE)
ttsumReqInfo2<-sum(dt$ttReqInfo2, na.rm=TRUE)
ttsumReqInfo3<-sum(dt$ttReqInfo3, na.rm=TRUE)

rcsumReqInfo1<-sum(dt$rcReqInfo1, na.rm=TRUE)
rcsumReqInfo2<-sum(dt$rcReqInfo2, na.rm=TRUE)
rcsumReqInfo3<-sum(dt$rcReqInfo3, na.rm=TRUE)

rtsumReqInfo1<-sum(dt$rtReqInfo1, na.rm=TRUE)
rtsumReqInfo2<-sum(dt$rtReqInfo2, na.rm=TRUE)
rtsumReqInfo3<-sum(dt$rtReqInfo3, na.rm=TRUE)


Number <- c("Other examples","Relative importance","Algorithm detail")
tc <- c(tcsumReqInfo1,tcsumReqInfo2,tcsumReqInfo3)
tt <- c(ttsumReqInfo1,ttsumReqInfo2,ttsumReqInfo3)
rc <- c(rcsumReqInfo1,rcsumReqInfo2,rcsumReqInfo3)
rt <- c(rtsumReqInfo1,rtsumReqInfo2,rtsumReqInfo3)
nyx <- data.frame(Number,tc,tt, rc, rt)

# reshape your data into long format
nyxlong <- melt(nyx, id=c("Number"))

# make the plot
ggplot(nyxlong) +
  geom_bar(aes(x = Number, y = value, fill = variable),
           stat="identity", position = "dodge", width = 0.7) +
  scale_fill_manual("Result\n", values = c("deepskyblue","blue4", "firebrick1","firebrick4"),
                    labels = c("Twitter control", "Twitter treatment","Recidivism control",
                               "Recidivism treatment")) +
  labs(x="\nAdditional Explanation",y="Result\n") +
  theme_bw(base_size = 14)

```

While not everyone answered this question, we see a consistent distribution of choices. The Relative Importance of factors was the most often selected in each combination of context and treatment. The Algorithm Details was a close second in many combinations of context and treatment, nearly the same as Relative Importance in the recidivism control. The control group almost always asked for more explanation than the treatment group, which can likely be attributed to the effectiveness of the explanation. A regression would show whether or not this is a significant effect. The only exception to this is that the Twitter treatment group selected Other Examples more than the control. This is a small difference in the least populated selection, and it is a smaller difference than the other options. 

###Not to include in report, but text outputs of Other q 4 
##Twitter control 
again, whether there's any oversight into the decision or any appeals (though in this specific case it was clearly valid)
Explanation of why they feel it's right to limit free speech.
Freedom of speech infringement, you can not like the guy but twitter shouldn't silence his viewpoint, individuals should block him. When you are a platform for social interaction you shouldn't be allowed to restrict who can say what.
None, I am against the restriction of the freedom of speech.
none, moderation should be done by humans
Statistics on pattern of behavior of banned individual, whether wrong people have ever been flagged
What conduct rule in particular was deemed violated by the algorithm.

##Twitter treatment
A contextual analysis of the actual subject of the comment
an explanation of the 70% threshold
Definition of the characteristics
Explanation of Sentiment Analysis
Explanation on what is sentiment analysis and how it was judged
I am perfectly satisfied with the explanation exactly as it is.
I'm satisfied with the information.
It should be stacked. All of the bars should add together. The way it's set up now, it could be at 69% threshold for all criteria and still would not have any action taken against it.
more detailed explanation of the graph
No other information required.
nothing, it is ridiculous
Proper spelling and explanation of sentiment decisions
What "sentiment analysis" means. Is this thing reacting to everything it views as expressing a negative or argumentative attitude?
when does using offensive vocabulary mean you are guilty, then pretty much all of us would be in jail and not got out of college
Why Twitter feels the need to implement an algorithm like this at all.

##Recidivism control
A human isn't simple enough, there are going to be many factors that won't be considered.
Again - riduculous just as previous answer - doesn't take "person" into account, just numbers
An explanation of why and how they use an algorithm in court--and who allowed it.
basically, it needs tons more information to make a decision like that
none
past success percentage
Statistics on accuracy
the specific information the algorithm uses to make the assessment
what happened to innocent until proven guilty?  you can not make assessment of someone based on algorithm when they did not do anything wrong
whether there's any human oversight into the decision

##Recidivism treatment
 A possibility of a human psychologist or social worker weighing in on the algorithm results too.
An explanation of how level of criminal personality was determined.
biases of those who wrote the algorithm
Definitions of each
How it can assume that everyone is the same based on answers.
human evaluation
I want to review the source code, the questions, the regression test results (when it was tested on repeat offenders)
I would like to know what information the algorithm considered when making the decision.
I'm already satisfied with the explanation.
More detailed breakdown of what is meant in this context by phrases like "criminal personality"
NONE
Numbers
records showing other uses that turned out to be accurate and the percentage of accuracy overall



